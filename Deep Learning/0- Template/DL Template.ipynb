{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac595392",
   "metadata": {},
   "source": [
    "# Steps of the Notebook\n",
    "<a id=\"toc\"></a>\n",
    "- [1. Template Description](#1)\n",
    "- [2. Import Libraries](#2)\n",
    "- [3. Pass the data path](#3)\n",
    "- [4. Adjust Diminsions of the Images](#4)\n",
    "- [5.Splitting the data into Training and Testing datasets](#5)\n",
    "- [6. Normalization](#6)\n",
    "- [7. One Hot Encoding](#7)\n",
    "- [8. Adjust Early Stop](#8)\n",
    "- [9. Adjust ANN and it's Parameters](#9)\n",
    "    - [9.1 Plot Loss Curve ANN](#9.1)\n",
    "    - [9.2 Plot Accuarcy Curve ANN](#9.2)\n",
    "    - [9.3 Plot Confusion Matrix ANN](#9.3)\n",
    "    - [9.4 Generate Classification Report](#9.4)\n",
    "- [10. Adjust CNN and it's Parameters](#10)\n",
    "    - [10.1 Plot Loss Curve CNN](#10.1)\n",
    "    - [10.2 Plot Accuarcy Curve CNN](#10.2)\n",
    "    - [10.3 Plot Confusion Matrix CNN](#10.3)\n",
    "    - [10.4 Generate Classification Report](#10.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fc6e9",
   "metadata": {},
   "source": [
    "# Template Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b497f91e",
   "metadata": {},
   "source": [
    "- **The template is good in dealing with images dataset. All you have to do is enter the path of the data and modify in some of the parameters of the neural networks (if you want to modify them) and you are done and it will do the training process, visualize loss and accuarcy curves, plot confusion matrix and generating classification Report and saves the best model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3e05c",
   "metadata": {},
   "source": [
    "- **The template supports images dataset only, and all images for each class must be placed in a separate folder**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147150db",
   "metadata": {},
   "source": [
    "- **For example, if I have two types of classification dogs and cats, you must put the pictures of cats in a folder and the pictures of dogs in another folder, then put these two folders in one folder, and then pass this folder**\n",
    "- **What is the Reason for That? it will automatically determine the number of classes in the data without the need for you to write them manually**<br><br>\n",
    "\n",
    "- **Real Example, Let's Back to the Cats and Dogs classification**<br>\n",
    "- **1 - Put Cats Images in a folder Called \"Cats\" (Name the Directory whatever you want I'm just assuming names)**<br>\n",
    "- **2 - Put Dogs Images in a folder Called \"Dogs\"**<br>\n",
    "- **3 - Then the two folders \"Cats and Dogs\" put them in a folder called \"Data\"**<br>\n",
    "- **4 - So \"Data\" Folder contains Cats Folder and Dogs Folder**<br>\n",
    "- **5 -The Last Step is Just Path the Path of the Data Folder in the 'data_path' variable**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ff01a",
   "metadata": {},
   "source": [
    "- **it's purpose is to save hours of your time**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e941fd",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1810496d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-29T20:36:19.277386Z",
     "start_time": "2022-01-29T20:36:19.266389Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from seaborn import heatmap\n",
    "import cv2\n",
    "import random\n",
    "from os import listdir\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import  LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array, array_to_img, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.math import confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132883e0",
   "metadata": {},
   "source": [
    "# Pass the Data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path and creating empty lists\n",
    "dir = data_path\n",
    "root_dir = listdir(dir)\n",
    "image_list, label_list = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b77266",
   "metadata": {},
   "source": [
    "# Adjust Image Diminsions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 128\n",
    "image_height = 128\n",
    "image_channel = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed319b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and converting image to numpy array\n",
    "for directory in root_dir:\n",
    "    for files in listdir(f\"{dir}/{directory}\"):\n",
    "        image_path = f\"{dir}/{directory}/{files}\"\n",
    "        image = load_img(image_path, target_size=(image_width, image_height))\n",
    "        image = img_to_array(image)\n",
    "        image_list.append(image)\n",
    "        label_list.append(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d114b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the number of classes count\n",
    "label_counts = pd.DataFrame(label_list).value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label_counts)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b6a95",
   "metadata": {},
   "source": [
    "# Splitting the data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba3c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.3, random_state = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c97746d",
   "metadata": {},
   "source": [
    "# Normlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and reshape data\n",
    "x_train = np.array(x_train, dtype=np.float16) / 225.0\n",
    "x_test = np.array(x_test, dtype=np.float16) / 225.0\n",
    "x_train = x_train.reshape( -1, image_width, image_height, image_channel)\n",
    "x_test = x_test.reshape( -1, image_width, image_height, image_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db69bda",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4028cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding on target variable\n",
    "LE = LabelEncoder()\n",
    "y_train = to_categorical(LE.fit_transform(y_train))\n",
    "y_test = to_categorical(LE.fit_transform(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed117906",
   "metadata": {},
   "source": [
    "# Adjust Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b63609",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(\n",
    "        patience=10,\n",
    "        min_delta=0,\n",
    "        monitor='val_loss',\n",
    "        restore_best_weights=True,\n",
    "        verbose=0,\n",
    "        mode='min', \n",
    "        baseline=None,\n",
    "    )\n",
    "plateau = callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.2, \n",
    "            patience=4, \n",
    "            verbose=0,\n",
    "            mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80f415",
   "metadata": {},
   "source": [
    "# Adjust ANN and it's Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer1 = 2048\n",
    "hidden_layer2 = 1024\n",
    "hidden_layer3 = 512\n",
    "hidden_layer4 = 256\n",
    "hidden_layer5 = 128\n",
    "hidden_layers_activation = 'relu'\n",
    "output_layer_activation = 'softmax'\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "loss_function = 'categorical_crossentropy'\n",
    "ANN_epochs = 1000\n",
    "batch_size = 10\n",
    "\n",
    "ANN_model_checkpoint = callbacks.ModelCheckpoint('ANN_best_model.h5', monitor='val_loss', mode='min', patience = 10 ,save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d569ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(): \n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(image_width,image_height,image_channel)),\n",
    "        Dense(hidden_layer1, activation =hidden_layers_activation),\n",
    "        Dense(hidden_layer2, activation =hidden_layers_activation),\n",
    "        Dense(hidden_layer3, activation =hidden_layers_activation),\n",
    "        Dense(hidden_layer4, activation =hidden_layers_activation),\n",
    "        Dense(hidden_layer5, activation =hidden_layers_activation),\n",
    "        Dense(num_classes, activation=output_layer_activation),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer= Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=loss_function,\n",
    "        metrics=['acc'],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "ann_model = load_model()\n",
    "history_ann = ann_model.fit(  x_train , y_train,\n",
    "                validation_data = (x_test , y_test),\n",
    "                epochs = ANN_epochs,\n",
    "                callbacks = [early_stopping , plateau, ANN_model_checkpoint],\n",
    "                batch_size = batch_size\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27daa8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980551ee",
   "metadata": {},
   "source": [
    "##  Loss Curve ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c14dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,8))\n",
    "loss_train = history_ann.history['loss']\n",
    "loss_val = history_ann.history['val_loss']\n",
    "epochs = range(1,len(loss_train) + 1)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749348e1",
   "metadata": {},
   "source": [
    "## Accuarcy Curve ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc533e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,8))\n",
    "val_train = history_ann.history['acc']\n",
    "val_acc = history_ann.history['val_acc']\n",
    "epochs = range(1,len(val_acc)+1)\n",
    "plt.plot(epochs, val_train, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='validation acc')\n",
    "plt.title('Training and Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b5954",
   "metadata": {},
   "source": [
    "## Confusion Matrix ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(y_test, y_pred):\n",
    "    y_predicted_labels = [np.argmax(i) for i in y_pred]\n",
    "    y_predicted_labels = np.array(y_predicted_labels)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    cm = confusion_matrix(labels=y_test, predictions=y_predicted_labels)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    heatmap(cm, annot=True, fmt='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f47a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ann = ann_model.predict(x_test)\n",
    "conf_matrix(y_test, y_pred_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5e660",
   "metadata": {},
   "source": [
    "## Classification Report ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_report(y_test, y_pred):\n",
    "    y_predicted_labels = [np.argmax(i) for i in y_pred]\n",
    "    y_predicted_labels = np.array(y_predicted_labels)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    print(classification_report(y_test, y_predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report(y_pred_ann, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266a5ec",
   "metadata": {},
   "source": [
    "# Adjust CNN and it's Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters1 = 16\n",
    "filters2 = 32\n",
    "filters3 = 64\n",
    "filters4 = 128\n",
    "\n",
    "kernel_size = (3,3)\n",
    "conv_activation = 'LeakyRelu'\n",
    "\n",
    "pool_size1 = (5,5)\n",
    "pool_size2 = (3,3)\n",
    "pool_size3 = (2,2)\n",
    "\n",
    "hidden_layer1 = 64\n",
    "hidden_layer2 = 32\n",
    "hidden_layer3 = 16\n",
    "\n",
    "hidden_layer_activation = 'relu'\n",
    "output_layer_activation = 'softmax'\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "loss_function = 'categorical_crossentropy'\n",
    "CNN_epochs = 1000\n",
    "\n",
    "CNN_model_checkpoint = callbacks.ModelCheckpoint('CNN_best_model.h5', monitor='val_loss', mode='min', patience = 10 ,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dbf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    cnn_model = Sequential([\n",
    "        Conv2D(filters1, kernel_size = kernel_size,input_shape=(image_width,image_height,image_channel), activation=conv_activation),\n",
    "        MaxPooling2D(pool_size=pool_size1),\n",
    "        Conv2D(filters2, kernel_size = kernel_size, activation=conv_activation),\n",
    "        MaxPooling2D(pool_size=pool_size1),\n",
    "        Conv2D(filters3, kernel_size = kernel_size, activation= conv_activation),\n",
    "        MaxPooling2D(pool_size=pool_size1),\n",
    "        Conv2D(filters4, kernel_size = kernel_size, activation= conv_activation),\n",
    "        MaxPooling2D(pool_size=pool_size1),\n",
    "        \n",
    "        Flatten(),\n",
    "        \n",
    "        Dense(hidden_layer1, activation =hidden_layer_activation),\n",
    "        Dense(hidden_layer2, activation =hidden_layer_activation),\n",
    "        Dense(hidden_layer3, activation =hidden_layer_activation),\n",
    "        Dense(num_classes, activation=output_layer_activation)    \n",
    "    ])\n",
    "    cnn_model.compile(\n",
    "        Adam(learning_rate=LEARNING_RATE),\n",
    "        loss = loss_function,\n",
    "        metrics = ['acc']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "cnn_model = load_model()\n",
    "history_cnn = cnn_model.fit(  x_train , y_train,\n",
    "                validation_data = (x_test , y_test),\n",
    "                epochs = CNN_epochs,\n",
    "                callbacks = [early_stopping , plateau, CNN_model_checkpoint],\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c9557",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0e699",
   "metadata": {},
   "source": [
    "## Loss Curve CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a029cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,8))\n",
    "loss_train = history_cnn.history['loss']\n",
    "loss_val = history_cnn.history['val_loss']\n",
    "epochs = range(1,len(loss_train) + 1)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabd1a3",
   "metadata": {},
   "source": [
    "## Accuarcy Curve CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd358bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,8))\n",
    "acc_train = history_cnn.history['acc']\n",
    "acc_val = history_cnn.history['val_acc']\n",
    "epochs = range(1,len(acc_train) + 1)\n",
    "plt.plot(epochs, acc_train, 'g', label='Training Accuarcy')\n",
    "plt.plot(epochs, acc_val, 'b', label='validation Accuarcy')\n",
    "plt.title('Training and Validation Accuarcy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf9bbd",
   "metadata": {},
   "source": [
    "## Confusion Matrix CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb140a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cnn = cnn_model.predict(x_test)\n",
    "conf_matrix(y_test, y_pred_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c32979",
   "metadata": {},
   "source": [
    "## Class Report CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86469151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report(y_test, y_pred_cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
